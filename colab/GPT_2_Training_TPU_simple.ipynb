{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training GPT-2 Using TPUs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/flancian/gpt-2/blob/tpu/colab/GPT_2_Training_TPU_simple.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIJikAQ_3CQQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f58b57ac-d7f2-48d6-ad42-e11c10962e88"
      },
      "source": [
        "# NOTE: Ensure \"Runtime -> Change runtime type\" is set to TPU.\n",
        "# Based on http://gwern.net/GPT-2 and shawwn's (thanks!).\n",
        "\n",
        "# Fetch the code.\n",
        "!git clone https://github.com/flancian/gpt-2 -b tpu /content/gpt-2\n",
        "import os\n",
        "os.chdir('/content/gpt-2')\n",
        "\n",
        "# Fetch the corpus.\n",
        "# This is just all of Virginia Woolf's works from Gutenberg AU, concatenated.\n",
        "!gsutil cp gs://botginia.appspot.com/woolf.txt .\n",
        "\n",
        "# install prerequisites.\n",
        "!pip install regex\n",
        "!pip install toposort\n",
        "!pip install fire\n",
        "!python download_model.py 117M\n",
        "\n",
        "\n",
        "\n",
        "# Note that there is currently no support for saving the trained model.\n",
        "# Theoretically it might work, but you'll have to create your own storage bucket and pass in --storage_bucket gs://your-bucket/gpt-2/\n",
        "\n",
        "# Also note that if you plan on increasing the context window on the larger models, you should pass --sample_length 1023. (It defaults to n_ctx - 1.)\n",
        "# The reason is because the sampling process gets exponentially slower as sample_length increases, so a large model\n",
        "# with a big sample_length might take hours to generate even one sample.\n",
        "\n",
        "# That said, it's interesting to train a 117M with --n_ctx 2048 --sample_length 2047 and watch it generate samples 2x longer than standard GPT-2.\n",
        "# (Sampling from 117M is *much* faster than the larger models, so you can get away with a larger sample_length during training.)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 117M\n",
        "!PYTHONPATH=src python ./train.py --dataset woolf.txt --batch_size 1 --save_every 100000 --sample_every 100 --init_tpu --fresh_model --n_ctx 1024 --n_embd 768 --n_head 12 --n_layer 12\n",
        "\n",
        "# 345M\n",
        "#!PYTHONPATH=src python ./train.py --dataset elisp-untab-eot-v001.txt.npz --batch_size 1 --save_every 100000 --sample_every 100 --init_tpu --fresh_model --n_ctx 1024 --n_embd 1024 --n_head 16 --n_layer 24\n",
        "\n",
        "# 1.5B\n",
        "# !PYTHONPATH=src python ./train.py --dataset elisp-untab-eot-v001.txt.npz --batch_size 1 --save_every 100000 --sample_every 100 --init_tpu --fresh_model --n_ctx 1024 --n_embd 1600 --n_head 25 --n_layer 48\n",
        "\n",
        "# 117M with 2x the context\n",
        "#!PYTHONPATH=src python ./train.py --dataset elisp-untab-eot-v001.txt.npz --batch_size 1 --save_every 100000 --sample_every 100 --init_tpu --fresh_model --n_ctx 2048 --n_embd 768 --n_head 12 --n_layer 12\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path '/content/gpt-2' already exists and is not an empty directory.\n",
            "Copying gs://botginia.appspot.com/woolf.txt...\n",
            "/ [1 files][  6.7 MiB/  6.7 MiB]                                                \n",
            "Operation completed over 1 objects/6.7 MiB.                                      \n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (2019.11.1)\n",
            "Requirement already satisfied: toposort in /usr/local/lib/python3.6/dist-packages (1.5)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire) (1.12.0)\n",
            "Fetching checkpoint: 1.00kit [00:00, 712kit/s]                                                      \n",
            "Fetching encoder.json: 1.04Mit [00:00, 51.9Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 586kit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:07, 70.3Mit/s]                                  \n",
            "Fetching model.ckpt.index: 6.00kit [00:00, 3.61Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 472kit [00:00, 50.7Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 46.8Mit/s]                                                       \n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:171: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/memory_saving_gradients.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/tflex.py:31: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/tflex.py:172: The name tf.train.SaverDef is deprecated. Please use tf.compat.v1.train.SaverDef instead.\n",
            "\n",
            "attach_debugger <function attach_debugger at 0x7fc792a72ea0>\n",
            "print_status <function print_status at 0x7fc792a72f28>\n",
            "freeze_forever <function freeze_forever at 0x7fc7929f6158>\n",
            "quit <function quit at 0x7fc7929f61e0>\n",
            "save_and_quit <function save_and_quit at 0x7fc7929f62f0>\n",
            "WARNING:tensorflow:From ./train.py:190: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "Using TPU grpc://10.126.218.202:8470\n",
            "Initializing TPU...\n",
            "WARNING:tensorflow:From ./train.py:196: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:173: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:20: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:21: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:177: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:178: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:45: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:49: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:191: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:67: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:16: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:72: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "This model is using 124439808 parameters (118.68M)\n",
            "WARNING:tensorflow:From ./train.py:231: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "main_command_set_learning_rate <function main.<locals>.set_learning_rate at 0x7fc7b8270598>\n",
            "WARNING:tensorflow:From ./train.py:266: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:304: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:307: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:309: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:320: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "Loading snapshot models/117M/model.ckpt...\n",
            "Loaded in 0.000009 seconds\n",
            "Loading dataset...\n",
            "Training...\n",
            "save <function main.<locals>.save at 0x7fc78c6f7048>\n",
            "main_command_generate_samples <function main.<locals>.generate_samples at 0x7fc78c6f70d0>\n",
            "main_command_validation <function main.<locals>.validation at 0x7fc78c6f7158>\n",
            "2019-12-01T07:15:05.381721-08:00 [1 | 0.6009] Running opt_apply...\n",
            "2019-12-01T07:15:11.264301-08:00 [1 | 6.4835 | 6.48s | 157.939053tokens/s] loss=10.9715 avg=10.9715 rate=0.0000200 step=0\n",
            "2019-12-01T07:15:11.316036-08:00 [2 | 6.5353] Running opt_apply...\n",
            "2019-12-01T07:15:13.880173-08:00 [2 | 9.0994 | 2.62s | 391.456227tokens/s] loss=10.5009 avg=10.7350 rate=0.0000200 step=1\n",
            "2019-12-01T07:15:13.924149-08:00 [3 | 9.1434] Running opt_apply...\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}